"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"HWU7RTJZ","conferencePaper","2023",", Youmi Ma; , An Wang; , Naoaki Okazaki","DREEAM: Guiding Attention with Evidence for Improving Document-Level Relation Extraction","Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics","","","10.18653/v1/2023.eacl-main.145","https://arxiv.org/abs/2302.08675v1","Document-level relation extraction (DocRE) is the task of identifying all relations between each entity pair in a document. Evidence, defined as sentences containing clues for the relationship between an entity pair, has been shown to help DocRE systems focus on relevant texts, thus improving relation extraction. However, evidence retrieval (ER) in DocRE faces two major issues: high memory consumption and limited availability of annotations. This work aims at addressing these issues to improve the usage of ER in DocRE. First, we propose DREEAM, a memory-efficient approach that adopts evidence information as the supervisory signal, thereby guiding the attention modules of the DocRE system to assign high weights to evidence. Second, we propose a self-training strategy for DREEAM to learn ER from automatically-generated evidence on massive data without evidence annotations. Experimental results reveal that our approach exhibits state-of-the-art performance on the DocRED benchmark for both DocRE and ER. To the best of our knowledge, DREEAM is the first approach to employ ER self-training.","2023-02-17","2023-10-30 16:40:45","2024-07-23 13:15:14","","","","","","","","DREEAM","","","","","ACL","","","NA","","https://paperswithcode.com/paper/dreeam-guiding-attention-with-evidence-for","https://github.com/youmima/dreeam","","[{""task"": ""Relation Extraction"", ""dataset"": ""DocRED"", ""res"": ""65.47"", ""metric"": ""Ign F1""}, {""task"": ""Relation Extraction"", ""dataset"": ""DocRED"", ""res"": ""67.53"", ""metric"": ""F1""}]","","","/root/snap/zotero-snap/common/Zotero/storage/XKBNUK6R/ et al. - 2023 - DREEAM Guiding Attention with Evidence for Improv.pdf","","PTM:Roberta; ARCHI:?; DATASET:?; LANG:?; NBDATASET:?; GRANULARITY:Document; PTM:Bert; COSTEVAL_BIN:?; DATATYPEPROP:?; INPUT:?; LINEARIZEDGRAPH_BIN:?; LOSSUPDATE_BIN:?; NBTYPEREL:?; OBJECTPROPERTIES_BIN:?; SYNTHGENERATION_BIN:?; USENEGATIVEEXAMPLE_BIN:?; LEARNINGMETHOD:?; DECODINGMETHOD_BIN:NSP; TASK:RelationIdentification; DatasetSplit:?","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","EACL","","","","","","","","","","","","","","",""
"ED67BIIS","conferencePaper","2021",", Tuan Lai; , Heng Ji; , ChengXiang Zhai; , Quan Hung Tran","Joint Biomedical Entity and Relation Extraction with Knowledge-Enhanced Collective Inference","","","","10.18653/v1/2021.acl-long.488","https://arxiv.org/abs/2105.13456v2","Compared to the general news domain, information extraction (IE) from biomedical text requires much broader domain knowledge. However, many previous IE methods do not utilize any external knowledge during inference. Due to the exponential growth of biomedical publications, models that do not go beyond their fixed set of parameters will likely fall behind. Inspired by how humans look up relevant information to comprehend a scientific text, we present a novel framework that utilizes external knowledge for joint entity and relation extraction named KECI (Knowledge-Enhanced Collective Inference). Given an input text, KECI first constructs an initial span graph representing its initial understanding of the text. It then uses an entity linker to form a knowledge graph containing relevant background knowledge for the the entity mentions in the text. To make the final predictions, KECI fuses the initial span graph and the knowledge graph into a more refined graph using an attention mechanism. KECI takes a collective approach to link mention spans to entities by integrating global relational information into local representations using graph convolutional networks. Our experimental results show that the framework is highly effective, achieving new state-of-the-art results in two different benchmark datasets: BioRelEx (binding interaction detection) and ADE (adverse drug event extraction). For example, KECI achieves absolute improvements of 4.59% and 4.91% in F1 scores over the state-of-the-art on the BioRelEx entity and relation extraction tasks.","2021-05-27","2023-10-30 16:40:45","2024-07-23 13:14:39","","NA","","","NA","","","BioRelex","","","","","ACL","","","NA","","https://paperswithcode.com/paper/joint-biomedical-entity-and-relation","https://github.com/laituan245/bio_relex","","","{'citing': ['10.1007/s11633-022-1323-6', '10.1109/access.2021.3130956', '10.1109/bibm52615.2021.9669360', '10.1016/j.knosys.2022.109129', '10.1109/itaic54216.2022.9836511'], 'cited': []}","","/root/snap/zotero-snap/common/Zotero/storage/6LZBUFZM/ et al. - 2021 - Joint Biomedical Entity and Relation Extraction wi.pdf","","ARCHI:?; DATASET:?; GRANULARITY:?; LANG:?; NBDATASET:?; PTM:?; TASK:Endtoendre; COSTEVAL_BIN:?; DATATYPEPROP:?; LINEARIZEDGRAPH_BIN:?; LOSSUPDATE_BIN:?; NBTYPEREL:?; OBJECTPROPERTIES_BIN:?; SYNTHGENERATION_BIN:?; USENEGATIVEEXAMPLE_BIN:?; LEARNINGMETHOD:?; INPUT:Graph; DECODINGMETHOD_BIN:NSP; TASK:RelationIdentification; DatasetSplit:?","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ACL 2021 5","","","","","","","","","","","","","","",""
"LMK8NN6J","conferencePaper","2022","Zhou, Yucheng; Shen, Tao; Geng, Xiubo; Long, Guodong; Jiang, Daxin","ClarET: Pre-training a Correlation-Aware Context-To-Event Transformer for Event-Centric Generation and Classification","Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2022.acl-long.183","https://aclanthology.org/2022.acl-long.183","Generating new events given context with correlated ones plays a crucial role in many eventcentric reasoning tasks. Existing works either limit their scope to specific scenarios or overlook event-level correlations. In this paper, we propose to pre-train a general Correlationaware context-to-Event Transformer (ClarET) for event-centric reasoning. To achieve this, we propose three novel event-centric objectives, i.e., whole event recovering, contrastive eventcorrelation encoding and prompt-based event locating, which highlight event-level correlations with effective training. The proposed ClarET is applicable to a wide range of eventcentric reasoning scenarios, considering its versatility of (i) event-correlation types (e.g., causal, temporal, contrast), (ii) application formulations (i.e., generation and classification), and (iii) reasoning types (e.g., abductive, counterfactual and ending reasoning). Empirical fine-tuning results, as well as zero- and fewshot learning, on 9 benchmarks (5 generation and 4 classification tasks covering 4 reasoning types with diverse event correlations), verify its effectiveness and generalization ability.","2022","2023-10-30 16:40:49","2024-07-23 13:15:24","2023-02-28 16:02:00","2559-2575","","","","","","ClarET","","","","","Association for Computational Linguistics","Dublin, Ireland","en","","","https://paperswithcode.com/paper/claret-pre-training-a-correlation-aware","","DOI.org (Crossref)","","","","/root/snap/zotero-snap/common/Zotero/storage/M9DF5U85/Zhou et al. - 2022 - ClarET Pre-training a Correlation-Aware Context-T.pdf","","ARCHI:?; DATASET:?; GRANULARITY:?; LANG:?; NBDATASET:?; PTM:Bart; COSTEVAL_BIN:?; DATATYPEPROP:?; INPUT:?; LINEARIZEDGRAPH_BIN:?; LOSSUPDATE_BIN:?; NBTYPEREL:?; OBJECTPROPERTIES_BIN:?; SYNTHGENERATION_BIN:?; USENEGATIVEEXAMPLE_BIN:?; LEARNINGMETHOD:Continual; LEARNINGMETHOD:Promptbased; DECODINGMETHOD_BIN:NSP; TASK:RelationClassif; TASK:RelationIdentification; DatasetSplit:?","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"RDH9G8RI","preprint","2023","Josifoski, Martin; Sakota, Marija; Peyrard, Maxime; West, Robert","Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction","","","","not found","http://arxiv.org/abs/2303.04132","Large language models (LLMs) show great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by the LLM: we show that, for problems with structured outputs, it is possible to prompt an LLM to perform the task in the opposite direction, to generate plausible text for the target structure. Leveraging the asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex tasks. We demonstrate the effectiveness of this approach on closed information extraction, where collecting ground-truth data is challenging, and no satisfactory dataset exists to date. We synthetically generate a dataset of 1.8M data points, demonstrate its superior quality compared to existing datasets in a human evaluation and use it to finetune small models (220M and 770M parameters). The models we introduce, SynthIE, outperform existing baselines of comparable size with a substantial gap of 57 and 79 absolute points in micro and macro F1, respectively. Code, data, and models are available at https://github.com/epfl-dlab/SynthIE.","2023-03-07","2024-04-12 12:12:08","2024-07-23 13:15:52","2023-03-20 09:04:52","","","","","","","SynthIE","","","","","","","","","","https://paperswithcode.com/paper/exploiting-asymmetry-for-synthetic-training","https://github.com/epfl-dlab/synthie","arXiv.org","","","","/root/snap/zotero-snap/common/Zotero/storage/SYZXIDNQ/Josifoski et al. - 2023 - Exploiting Asymmetry for Synthetic Training Data G.pdf","","LANG:?; NBDATASET:?; GRANULARITY:Sentences; ARCHI:Encoder-Decoder; PTM:Flant5; COSTEVAL_BIN:?; DATATYPEPROP:?; INPUT:?; LINEARIZEDGRAPH_BIN:?; LOSSUPDATE_BIN:?; NBTYPEREL:?; OBJECTPROPERTIES_BIN:?; SYNTHGENERATION_BIN:?; DATASET:Rebel; LEARNINGMETHOD:Finetuning; USENEGATIVEEXAMPLE_BIN:0; DECODINGMETHOD_BIN:NSP; TASK:RelationIdentification; DatasetSplit:?","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JX4IQYET","conferencePaper","2022","Josifoski, Martin; De Cao, Nicola; Peyrard, Maxime; Petroni, Fabio; West, Robert","GenIE: Generative Information Extraction","Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","","","10.18653/v1/2022.naacl-main.342","https://aclanthology.org/2022.naacl-main.342","US president Biden was born in Pennsylvania.","2022","2024-04-12 12:14:59","2024-07-23 13:15:55","2023-05-02 07:12:23","4626-4643","","","","","","GenIE","","","","","ACL","Seattle, United States","en","","","https://paperswithcode.com/paper/genie-generative-information-extraction","https://github.com/epfl-dlab/genie","DOI.org (Crossref)","","{'citing': ['10.1007/978-3-031-19433-7_47'], 'cited': []}","","/root/snap/zotero-snap/common/Zotero/storage/DTDTPI3A/Josifoski et al. - 2022 - GenIE Generative Information Extraction.pdf","","LANG:?; NBDATASET:?; GRANULARITY:Sentences; ARCHI:Encoder-Decoder; PTM:Bart; DATASET:Fewrel; COSTEVAL_BIN:?; DATATYPEPROP:?; INPUT:?; LOSSUPDATE_BIN:?; NBTYPEREL:?; OBJECTPROPERTIES_BIN:?; SYNTHGENERATION_BIN:?; USENEGATIVEEXAMPLE_BIN:?; DATASET:Rebel; LEARNINGMETHOD:Finetuning; LINEARIZEDGRAPH_BIN:1; DECODINGMETHOD_BIN:NSP; TASK:RelationIdentification; DatasetSplit:?","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","","","","","","","","","","","","","","",""
"CKI2K2PE","conferencePaper","2020","Wang, Jue; Lu, Wei","Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders","Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)","","","10.18653/v1/2020.emnlp-main.133","https://www.aclweb.org/anthology/2020.emnlp-main.133","","2020","2024-04-12 12:16:28","2024-07-23 13:15:57","2023-02-28 16:15:26","1706-1721","","","","","","Table-Sequence","","","","","ACL","Online","en","","","https://paperswithcode.com/paper/two-are-better-than-one-joint-entity-and","https://github.com/LorrinWWW/two-are-better-than-one","DOI.org (Crossref)","[{""task"": ""Relation Extraction"", ""dataset"": ""ACE 2005"", ""res"": ""64.3"", ""metric"": ""RE Micro F1""}, {""task"": ""Relation Extraction"", ""dataset"": ""Adverse Drug Events (ADE) Corpus"", ""res"": ""80.1"", ""metric"": ""RE Macro F1""}, {""task"": ""Relation Extraction"", ""dataset"": ""ACE 2004"", ""res"": ""59.6"", ""metric"": ""RE+ Micro F1""}, {""task"": ""Relation Extraction"", ""dataset"": ""ACE 2004"", ""res"": ""88.6"", ""metric"": ""NER Micro F1""}, {""task"": ""Relation Extraction"", ""dataset"": ""ACE 2004"", ""res"": ""59.6"", ""metric"": ""RE Micro F1""}, {""task"": ""Relation Extraction"", ""dataset"": ""ACE 2005"", ""res"": ""64.3"", ""metric"": ""RE+ Micro F1""}, {""task"": ""Relation Extraction"", ""dataset"": ""ACE 2005"", ""res"": ""No"", ""metric"": ""Cross Sentence""}, {""task"": ""Relation Extraction"", ""dataset"": ""ACE 2005"", ""res"": ""ALBERT"", ""metric"": ""Sentence Encoder""}, {""task"": ""Relation Extraction"", ""dataset"": ""Adverse Drug Events (ADE) Corpus"", ""res"": ""89.7"", ""metric"": ""NER Macro F1""}, {""task"": ""Relation Extraction"", ""dataset"": ""ACE 2005"", ""res"": ""67.6"", ""metric"": ""RE Micro F1""}, {""task"": ""Relation Extraction"", ""dataset"": ""ACE 2004"", ""res"": ""No"", ""metric"": ""Cross Sentence""}, {""task"": ""Relation Extraction"", ""dataset"": ""ACE 2004"", ""res"": ""63.3"", ""metric"": ""RE+ Micro F1""}, {""task"": ""Relation Extraction"", ""dataset"": ""Adverse Drug Events (ADE) Corpus"", ""res"": ""80.1"", ""metric"": ""RE+ Macro F1""}, {""task"": ""Relation Extraction"", ""dataset"": ""ACE 2004"", ""res"": ""63.3"", ""metric"": ""RE Micro F1""}, {""task"": ""Relation Extraction"", ""dataset"": ""ACE 2005"", ""res"": ""89.5"", ""metric"": ""NER Micro F1""}, {""task"": ""Relation Extraction"", ""dataset"": ""ACE 2005"", ""res"": ""67.6"", ""metric"": ""RE+ Micro F1""}, {""task"": ""Zero-shot Relation Triplet Extraction"", ""dataset"": ""Wiki-ZSL"", ""res"": ""6.4"", ""metric"": ""Avg. F1""}, {""task"": ""Zero-shot Relation Triplet Extraction"", ""dataset"": ""FewRel"", ""res"": ""6.37"", ""metric"": ""Avg. F1""}]","{'citing': ['10.1007/978-981-16-5188-5_2', '10.1007/s12559-021-09917-7', '10.1016/j.jii.2021.100301', '10.1007/978-3-030-88483-3_37', '10.1016/j.compag.2022.106776', '10.1007/978-3-030-93733-1_23', '10.1080/09540091.2022.2026295', '10.1162/coli_a_00415', '10.5715/jnlp.29.187', '10.1007/s10115-022-01665-w', '10.1007/s11633-022-1323-6', '10.1016/j.neucom.2022.04.059', '10.1109/iccc54389.2021.9674535', '10.1109/cscwd54268.2022.9776159', '10.3390/app12126231', '10.1016/j.knosys.2022.109129', '10.1109/bigdata52589.2021.9671514', '10.1145/3530190.3534850', '10.1093/bib/bbac342', '10.1007/978-3-031-17120-8_21', '10.1145/3554734', '10.1080/10095020.2022.2076619', '10.1109/icpr56361.2022.9956191', '10.1016/j.jksuci.2022.08.038', '10.1007/978-3-031-18315-7_11', '10.1007/978-981-19-6142-7_10'], 'cited': []}","","/root/snap/zotero-snap/common/Zotero/storage/DTNJCXYZ/Wang and Lu - 2020 - Two are Better than One Joint Entity and Relation.pdf","","PTM:Roberta; NBDATASET:?; ARCHI:Cnn; GRANULARITY:Document; LANG:Arabic; LANG:Chinese; LANG:English; PTM:Albert; DATASET:Ace2005; DATASET:Fewrel; COSTEVAL_BIN:?; DATATYPEPROP:?; INPUT:?; LINEARIZEDGRAPH_BIN:?; LOSSUPDATE_BIN:?; NBTYPEREL:?; OBJECTPROPERTIES_BIN:?; SYNTHGENERATION_BIN:?; USENEGATIVEEXAMPLE_BIN:?; DATASET:Ade; LEARNINGMETHOD:Finetuning; DATASET:Wiki-Zsl; DATASET:Conll; DECODINGMETHOD_BIN:NSP; TASK:RelationIdentification; DatasetSplit:?","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)","","","","","","","","","","","","","","",""
"5AP3EIC4","journalArticle","2019","Pang, Yihe; Liu, Jie; Liu, Lizhen; Yu, Zhengtao; Zhang, Kai","A Deep Neural Network Model for Joint Entity and Relation Extraction","IEEE Access","","2169-3536","10.1109/ACCESS.2019.2949086","https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8880633&tag=1","Joint extraction of entities and their relations from the text is an essential issue in automatic knowledge graph construction, which is also known as the joint extraction of relational triplets. The relational triplets in sentence are complicated, multiple and different relational triplets may have overlaps, which is commonly seen in reality. However, multiple pairs of triplets cannot be efficiently extracted in most of the previous works. To mitigate this problem, we propose a deep neural network model based on the sequence-to-sequence learning, namely, the hybrid dual pointer networks (HDP), which extracts multiple pairs of triplets from the given sentence by generating the hybrid dual pointer sequence. In experiments, we tested our model using the New York Times (NYT) public dataset. The experimental results demonstrated that our model outperformed the state-of-the-art work, and achieved a 17.1% improvement on the F1 values.","2019","2025-03-04 10:35:17","2025-03-04 10:36:35","","179143-179150","","","7","","","","","","","","","","","","","https://paperswithcode.com/paper/end-to-end-neural-relation-extraction-using","","IEEE Xplore","[{""task"": ""Named Entity Recognition (NER)"", ""dataset"": ""JNLPBA"", ""res"": ""82.0"", ""metric"": ""F1""}, {""task"": ""Named Entity Recognition (NER)"", ""dataset"": ""BC2GM"", ""res"": ""85.1"", ""metric"": ""F1""}, {""task"": ""Relation Extraction"", ""dataset"": ""GAD"", ""res"": ""84.3"", ""metric"": ""F1""}, {""task"": ""Relation Extraction"", ""dataset"": ""DDI"", ""res"": ""81.9"", ""metric"": ""F1""}, {""task"": ""Relation Extraction"", ""dataset"": ""ChemProt"", ""res"": ""77.5"", ""metric"": ""F1""}]","{'citing': ['10.1093/bioinformatics/btaa667', '10.3390/app10186429', '10.1016/j.jbi.2021.103761', '10.1093/bib/bbac342', '10.1007/s10462-022-10239-9', '10.3390/app12199691'], 'cited': ['10.1016/j.neucom.2016.12.075', '10.1145/2736277.2741093', '10.1609/aaai.v33i01.33017080', '10.18653/v1/d15-1205', '10.18653/v1/d17-1181', '10.18653/v1/d18-1307', '10.18653/v1/p16-1014', '10.18653/v1/p16-1105', '10.18653/v1/p16-1123', '10.18653/v1/p17-1085', '10.18653/v1/p17-1099', '10.18653/v1/p17-1113', '10.18653/v1/p18-1047', '10.24963/ijcai.2018/620', '10.3115/1596374.1596399', '10.3115/v1/d14-1200', '10.3115/v1/p14-1038']}","","","","DATASET:?; NBDATASET:?; PTM:?; GRANULARITY:Sentences; ARCHI:Bilstm; TASK:Endtoendre; LANG:English; COSTEVAL_BIN:?; DATATYPEPROP:?; LINEARIZEDGRAPH_BIN:?; LOSSUPDATE_BIN:?; OBJECTPROPERTIES_BIN:?; SYNTHGENERATION_BIN:?; USENEGATIVEEXAMPLE_BIN:?; LEARNINGMETHOD:?; INPUT:Text; ARCHI:Encoder; NBTYPEREL:10^1; DECODINGMETHOD_BIN:NSP; TASK:RelationIdentification; DatasetSplit:?","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8572UGFC","conferencePaper","2019",", Kalpit Dixit; , Yaser Al-Onaizan","Span-Level Model for Relation Extraction","","","","10.18653/v1/p19-1525","https://aclanthology.org/P19-1525","Relation Extraction is the task of identifying entity mention spans in raw text and then identifying relations between pairs of the entity mentions. Recent approaches for this span-level task have been token-level models which have inherent limitations. They cannot easily define and implement span-level features, cannot model overlapping entity mentions and have cascading errors due to the use of sequential decoding. To address these concerns, we present a model which directly models all possible spans and performs joint entity mention detection and relation extraction. We report a new state-of-the-art performance of 62.83 F1 (prev best was 60.49) on the ACE2005 dataset.","2019-07-01","2025-03-04 10:36:39","2025-03-04 10:38:54","","NA","","","NA","","","Span-level","","","","","ACL","","","NA","","https://paperswithcode.com/paper/span-level-model-for-relation-extraction","na","","[{""task"": ""Relation Extraction"", ""dataset"": ""ACE 2005"", ""res"": ""ELMo"", ""metric"": ""Sentence Encoder""}, {""task"": ""Relation Extraction"", ""dataset"": ""ACE 2005"", ""res"": ""85.98"", ""metric"": ""NER Micro F1""}, {""task"": ""Relation Extraction"", ""dataset"": ""ACE 2005"", ""res"": ""No"", ""metric"": ""Cross Sentence""}, {""task"": ""Relation Extraction"", ""dataset"": ""ACE 2005"", ""res"": ""62.83"", ""metric"": ""RE Micro F1""}]","{'citing': ['10.1093/bioinformatics/btaa993', '10.1016/j.neucom.2020.09.044', '10.1007/978-3-030-70665-4_203', '10.1007/978-981-16-2502-2_52', '10.1016/j.ipm.2021.102563', '10.2196/preprints.25670', '10.2196/25670', '10.1016/j.knosys.2021.107298', '10.1007/978-3-030-84529-2_39', '10.1007/978-3-030-89363-7_6', '10.1145/3508546.3508639', '10.1162/coli_a_00415', '10.3934/mbe.2022240', '10.1016/j.neucom.2022.04.059', '10.1016/j.neucom.2022.04.022', '10.1109/ijcnn48605.2020.9207021', '10.1007/978-3-031-10986-7_7', '10.1109/taai54685.2021.00012', '10.1109/ispa-bdcloud-socialcom-sustaincom51426.2020.00090', '10.1109/icicn52636.2021.9673905', '10.1145/3511808.3557459', '10.1145/3540250.3549089', '10.1016/j.jksuci.2022.08.038', '10.1007/978-3-031-18315-7_11', '10.1007/978-981-19-5391-0_6'], 'cited': []}","","","","ARCHI:?; PTM:?; GRANULARITY:Document; ARCHI:Bilstm; TASK:Endtoendre; LANG:Arabic; LANG:Chinese; LANG:English; DATASET:Ace2005; COSTEVAL_BIN:?; INPUT:?; LOSSUPDATE_BIN:?; NBTYPEREL:?; OBJECTPROPERTIES_BIN:?; SYNTHGENERATION_BIN:?; LEARNINGMETHOD:?; DATATYPEPROP:String; USENEGATIVEEXAMPLE_BIN:0; LINEARIZEDGRAPH_BIN:0; NBDATASET:1; DECODINGMETHOD_BIN:NSP; DATATYPEPROP:0; DatasetSplit:?","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ACL 2019 7","","","","","","","","","","","","","","",""
"J6J3TSGW","conferencePaper","2021",", Ningyu Zhang; , Xiang Chen; , Xin Xie; , Shumin Deng; , Chuanqi Tan; , Mosha Chen; , Fei Huang; , Luo Si; , Huajun Chen","Document-level Relation Extraction as Semantic Segmentation","Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence","","","10.24963/ijcai.2021/551","https://arxiv.org/abs/2106.03618v2","Document-level relation extraction aims to extract relations among multiple entity pairs from a document. Previously proposed graph-based or transformer-based models utilize the entities independently, regardless of global information among relational triples. This paper approaches the problem by predicting an entity-level relation matrix to capture local and global information, parallel to the semantic segmentation task in computer vision. Herein, we propose a Document U-shaped Network for document-level relation extraction. Specifically, we leverage an encoder module to capture the context information of entities and a U-shaped segmentation module over the image-style feature map to capture global interdependency among triples. Experimental results show that our approach can obtain state-of-the-art performance on three benchmark datasets DocRED, CDR, and GDA.","2021-06-07","2025-03-04 10:38:59","2025-03-04 10:40:09","","","","","","","","DocuNet","","","","","IJCAI","","","NA","","https://paperswithcode.com/paper/document-level-relation-extraction-as","https://github.com/wutong8023/Awesome_Information_Extraction","","[{""task"": ""Relation Extraction"", ""dataset"": ""GDA"", ""res"": ""85.3"", ""metric"": ""F1""}, {""task"": ""Relation Extraction"", ""dataset"": ""DocRED"", ""res"": ""62.4"", ""metric"": ""Ign F1""}, {""task"": ""Relation Extraction"", ""dataset"": ""DocRED"", ""res"": ""64.55"", ""metric"": ""F1""}, {""task"": ""Relation Extraction"", ""dataset"": ""CDR"", ""res"": ""76.3"", ""metric"": ""F1""}]","{'citing': ['10.1145/3404835.3462908', '10.1007/s00521-021-06667-3', '10.1145/3508546.3508633', '10.1145/3485447.3511921', '10.1145/3485447.3511998', '10.1007/s00521-022-07223-3', '10.1145/3477495.3531992', '10.1145/3477495.3531746', '10.1109/icme52920.2022.9859701', '10.1007/978-3-031-17120-8_13', '10.1016/j.eswa.2022.117678', '10.1145/3511808.3557313', '10.1109/icpr56361.2022.9956191', '10.1109/ijcnn55064.2022.9892647'], 'cited': []}","","","","ARCHI:?; DATASET:?; NBDATASET:?; GRANULARITY:Document; PTM:Bert; LANG:English; COSTEVAL_BIN:?; DATATYPEPROP:?; LINEARIZEDGRAPH_BIN:?; LOSSUPDATE_BIN:?; NBTYPEREL:?; OBJECTPROPERTIES_BIN:?; SYNTHGENERATION_BIN:?; USENEGATIVEEXAMPLE_BIN:?; LEARNINGMETHOD:?; DECODINGMETHOD_BIN:NSP; INPUT:TEXT; TASK:RelationClassif; DatasetSplit:?","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","IJCAI","","","","","","","","","","","","","","",""
"F3EBUUM7","conferencePaper","2020",", Hao Peng; , Tianyu Gao; , Xu Han; , Yankai Lin; , Peng Li; , Zhiyuan Liu; , Maosong Sun; , Jie zhou","Learning from Context or Names? An Empirical Study on Neural Relation Extraction","","","","10.18653/v1/2020.emnlp-main.298","https://arxiv.org/abs/2010.01923v2","Neural models have achieved remarkable success on relation extraction (RE) benchmarks. However, there is no clear understanding which type of information affects existing RE models to make decisions and how to further improve the performance of these models. To this end, we empirically study the effect of two main information sources in text: textual context and entity mentions (names). We find that (i) while context is the main source to support the predictions, RE models also heavily rely on the information from entity mentions, most of which is type information, and (ii) existing datasets may leak shallow heuristics via entity mentions and thus contribute to the high performance on RE benchmarks. Based on the analyses, we propose an entity-masked contrastive pre-training framework for RE to gain a deeper understanding on both textual context and type information while avoiding rote memorization of entities or use of superficial cues in mentions. We carry out extensive experiments to support our views, and show that our framework can improve the effectiveness and robustness of neural models in different RE scenarios. All the code and datasets are released at https://github.com/thunlp/RE-Context-or-Names.","2020-10-05","2025-03-19 14:44:30","2025-03-19 14:45:25","","NA","","","NA","","","Contrastive Pre-training","","","","","ACL","","","NA","","https://paperswithcode.com/paper/learning-from-context-or-names-an-empirical","https://github.com/thunlp/RE-Context-or-Names","","{""TACRED"": {""F1"": ""69.5""}}","{'citing': ['10.1007/978-3-030-84186-7_13', '10.1145/3462244.3479895', '10.1007/978-3-030-88480-2_23', '10.3934/mbe.2022240', '10.1007/s10489-022-03210-2', '10.1007/s00500-022-07195-5', '10.1007/s10489-022-03596-z', '10.1109/taslp.2022.3161146', '10.1145/3477495.3531789', '10.1145/3511808.3557313', '10.1145/3511808.3557459', '10.3390/electronics11223715', '10.1007/978-3-031-10983-6_31', '10.1007/978-3-031-18315-7_7'], 'cited': []}","","","","DONE; DATASET:?; NBDATASET:?; ARCHI:Cnn; GRANULARITY:Sentences; PTM:Bert; LANG:English; LOSSUPDATE_BIN:?; NBTYPEREL:?; OBJECTPROPERTIES_BIN:?; SYNTHGENERATION_BIN:?; USENEGATIVEEXAMPLE_BIN:?; INPUT:Text; LEARNINGMETHOD:Finetuning; COSTEVAL_BIN:0; DECODINGMETHOD_BIN:0; DATATYPEPROP:NSP; LINEARIZEDGRAPH_BIN:0; PTM:MTB; TASK:RelationIdentification; DatasetSplit:?","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","EMNLP 2020 11","","","","","","","","","","","","","","",""